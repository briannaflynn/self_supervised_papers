TITLE,JOURNAL,LINK,DATE,UK BIOBANK,SUMMARY,NOTES
Generalized radiograph representation learning via cross-supervision between images and free-text radiology reports,Nature Machine Intelligence,https://www.nature.com/articles/s42256-021-00425-9,1/20/22,FALSE,"""Pre-training lays the foundation for recent successes in radiograph analysis supported by deep learning. It learns transferable image representations by conducting large-scale fully- or self-supervised learning on a source domain; however, supervised pre-training requires a complex and labour-intensive two-stage human-assisted annotation process, whereas self-supervised learning cannot compete with the supervised paradigm. To tackle these issues, we propose a cross-supervised methodology called reviewing free-text reports for supervision (REFERS), which acquires free supervision signals from the original radiology reports accompanying the radiographs. The proposed approach employs a vision transformer and is designed to learn joint representations from multiple views within every patient study. REFERS outperforms its transfer learning and self-supervised learning counterparts on four well-known X-ray datasets under extremely limited supervision. Moreover, REFERS even surpasses methods based on a source domain of radiographs with human-assisted structured labels; it therefore has the potential to replace canonical pre-training methodologies.""","Uses a vision transformer - ""cross supervision"" approach means acquiring labels for supervised training from the radiology reports themselves, what they call ""free-text"""
Pushing the limits of self-supervised ResNets: Can we outperform supervised learning without labels on ImageNet?,arXiv,https://arxiv.org/pdf/2201.05119.pdf,1/13/22,FALSE,,
GestaltMatcher facilitates rare disease matching using facial phenotype descriptors,Nature Genetics,https://www.nature.com/articles/s41588-021-01010-x,2/10/22,FALSE,,
Bootstrap your own latent: A new approach to self-supervised Learning,arXiv,https://arxiv.org/abs/2006.07733,9/10/20,FALSE,,
A Simple Framework for Contrastive Learning of Visual Representations,arXiv,https://arxiv.org/pdf/2002.05709.pdf,7/1/20,FALSE,"In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-ofthe-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100× fewer labels.","This is one of the foundational papers in contrastive learning from Geoffrey Hinton, ReLICv2 uses this main idea as the backbone for it's approach."
Self-Supervised Multi-Modal Alignment for Whole Body Medical Imaging,arXiv,,8/6/21,TRUE,"This paper explores the use of self-supervised deep learning in medical imaging in cases where two scan modalities are available for the same subject. Specifically, we use a large publicly-available dataset of over 20,000 subjects from the UK Biobank with both whole body Dixon technique magnetic resonance (MR) scans and also dual-energy x-ray absorptiometry (DXA) scans. We make three contributions: (i) We introduce a multi-modal image-matching contrastive framework, that is able to learn to match different-modality scans of the same subject with high accuracy. (ii) Without any adaption, we show that the correspondences learnt during this contrastive training step can be used to perform automatic cross-modal scan registration in a completely unsupervised manner. (iii) Finally, we use these registrations to transfer segmentation maps from the DXA scans to the MR scans where they are used to train a network to segment anatomical regions without requiring ground-truth MR examples. To aid further research, our code will be made publicly available.","This is interesting because it uses a self-supervised approach on UKB DXA and MR scans, for the purpose to matching patients with multiple imaging modalities available. Not using for the same usecase of identifying rare phenotypes and validating with genetics"