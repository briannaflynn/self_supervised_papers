TITLE,JOURNAL,LINK,GITHUB,DATE,SUMMARY,NOTES,PAPER_EXPLAINED,
Learning Transferable Visual Models From Natural Language Supervision,arXiv,https://arxiv.org/abs/2103.00020,,2021,"This paper introduces CLIP (Contrastive Language‚ Image Pretraining), a neural network trained on a variety of images and texts. CLIP learns visual concepts from natural language supervision, enabling it to perform a wide range of visual tasks without task-specific training.",,,
Best of Both Worlds: Multimodal Contrastive Learning with Tabular and Imaging Data,arXiv,https://arxiv.org/abs/2303.14080,,,"This paper proposes a self-supervised contrastive learning framework combining images and tabular data for training unimodal encoders, addressing the challenges in medical datasets with limited diversity and scale. It integrates SimCLR and SCARF strategies, demonstrating its effectiveness in predicting myocardial infarction and CAD risks using cardiac MR images and clinical features. The study highlights the importance of morphometric tabular features in the learning process and introduces a novel supervised contrastive learning approach, 'label as a feature' (LaaF), which outperforms other baselines.",,,
Self-Supervised Multi-Modal Alignment for Whole Body Medical Imaging,arXiv,https://arxiv.org/abs/2107.06652,,2021,"This paper delves into self-supervised deep learning in medical imaging, specifically using over 20,000 subjects' MR and DXA scans from the UK Biobank. It introduces a multi-modal image-matching contrastive framework for accurate matching of different-modality scans. Additionally, it demonstrates the use of learned correspondences for unsupervised cross-modal scan registration and transferring segmentation maps from DXA to MR scans for anatomical region segmentation without ground-truth MR examples.",,,
Deep learning predicts all-cause mortality from longitudinal total-body DXA imaging,Nature communications medicine,https://www.nature.com/articles/s43856-022-00166-9,,2022,"This research utilizes deep learning to analyze longitudinal total-body DXA imaging for predicting all-cause mortality, extending previous biomarker studies. It examines two hypotheses: one, features from total-body DXA imaging predict mortality with/without clinical factors; two, sequential DXA scans and recurrent neural network models are superior to single-observation models in mortality prediction, with/without clinical factors.",,,
Contrastive Learning of Medical Visual Representations from Paired Images and Text,arXiv,https://arxiv.org/abs/2010.00747,,2020,"This paper introduces ConVIRT, a novel unsupervised strategy for learning medical visual representations by leveraging naturally occurring descriptive text paired with medical images. It challenges the traditional reliance on ImageNet pretraining and rule-based label extraction, proposing a bidirectional contrastive objective between image and text modalities. ConVIRT, tested on various medical image classification and zero-shot retrieval tasks, demonstrates significantly improved performance and superior data efficiency, requiring far less labeled training data compared to ImageNet initialized counterparts.",,,
Eye-brain connections revealed by multimodal retinal and brain imaging genetics in the UK Biobank,medrxiv,https://www.medrxiv.org/content/10.1101/2023.02.16.23286035v1,,2023,"This study conducts a genetic analysis of eye-brain connections using retinal and brain imaging data. It uncovers new links between retinal imaging biomarkers and brain structure/function, highlighting genetic overlaps with neurological and neuropsychiatric disorders like Alzheimer‚Äôs. The findings suggest retinal imaging can reveal genetic risks for brain disorders and changes in intracranial structure/function.",,,
Using deep learning to analyze the compositeness of musculoskeletal aging reveals that spine  hip and knee age at different rates and are associated with different genetic and non-genetic factors,medrxiv,https://www.medrxiv.org/content/10.1101/2021.06.14.21258896v1.full,,2021,"Utilizing deep learning and 42,000 X-ray images from the UK Biobank, this study examines the aging process of the musculoskeletal system. It finds that the spine, hip, and knee age at varying rates, influenced by unique genetic and non-genetic factors. The research highlights moderate correlations in accelerated aging across musculoskeletal dimensions, and identifies specific genetic markers and environmental variables linked to aging in each dimension.",,,
Personal Omics for Precision Health,Circulation (American Heart Association),https://www.ahajournals.org/doi/10.1161/CIRCRESAHA.117.310909,,,"This article explores the potential of personal omics in shaping future data-centric healthcare models. It discusses the integration of individualized health data across biomolecular, physiological, and environmental dimensions, focusing on early disease detection and lifestyle/environmental pattern analysis. The paper also addresses the challenges in aggregating, integrating, and protecting personal omics data for enhancing disease understanding, data-driven clinical decisions, and motivating behavioral change.",,,
A global overview of pleiotropy and genetic architecture in complex traits,Nature Genetics,https://www.nature.com/articles/s41588-019-0481-0,,,"Analyzing 4,155 GWASs, this study offers a comprehensive view of pleiotropy and genetic architecture in complex traits. It emphasizes that trait-associated loci, covering over half the genome, often overlap with multiple traits. The research highlights the enrichment of causal variants in key genetic regions and shows varied polygenicity and discoverability across traits, offering insights into how genetic variation influences trait variation.",,,
Multimodal biomedical AI,Nature Medicine,https://www.nature.com/articles/s41591-022-01981-2,,,This review highlights the rise of multimodal AI in biomedicine,,,
Generalized radiograph representation learning via cross-supervision between images and free-text radiology reports,Nature Machine Intelligence,https://www.nature.com/articles/s42256-021-00425-9,,1/20/22,"""Pre-training lays the foundation for recent successes in radiograph analysis supported by deep learning. It learns transferable image representations by conducting large-scale fully- or self-supervised learning on a source domain; however, supervised pre-training requires a complex and labour-intensive two-stage human-assisted annotation process, whereas self-supervised learning cannot compete with the supervised paradigm. To tackle these issues, we propose a cross-supervised methodology called reviewing free-text reports for supervision (REFERS), which acquires free supervision signals from the original radiology reports accompanying the radiographs. The proposed approach employs a vision transformer and is designed to learn joint representations from multiple views within every patient study. REFERS outperforms its transfer learning and self-supervised learning counterparts on four well-known X-ray datasets under extremely limited supervision. Moreover, REFERS even surpasses methods based on a source domain of radiographs with human-assisted structured labels; it therefore has the potential to replace canonical pre-training methodologies.""","Uses a vision transformer - ""cross supervision"" approach means acquiring labels for supervised training from the radiology reports themselves, what they call ""free-text""",,
Pushing the limits of self-supervised ResNets: Can we outperform supervised learning without labels on ImageNet?,arXiv,https://arxiv.org/pdf/2201.05119.pdf,,1/13/22,,,,
GestaltMatcher facilitates rare disease matching using facial phenotype descriptors,Nature Genetics,https://www.nature.com/articles/s41588-021-01010-x,,2/10/22,,,,
Bootstrap your own latent: A new approach to self-supervised Learning,arXiv,https://arxiv.org/abs/2006.07733,,9/10/20,,,,
Momentum Contrast for Unsupervised Visual Representation Learning,arXIv,https://arxiv.org/pdf/1911.05722.pdf,https://github.com/facebookresearch/moco,3/23/20,"""Pre-training lays the foundation for recent successes in radiograph analysis supported by deep learning. It learns transferable image representations by conducting large-scale fully- or self-supervised learning on a source domain; however, supervised pre","ReLIC architecture incorporates features of this method - ""The keys are encoded by a slowly progressing
encoder, driven by a momentum update with the query encoder."" Since the goal of the model is learn to differentiate between a large number of different images, this query image encoding is not only compared to one mini-batch of encoded key images, but to multiple of them. To achieve that, MoCo forms a queue of mini-batches that are encoded by the momentum encoder network. | The loss resembles a softmax-based classfier loss that aims to classify dictionary query as key of query. The sum is computed over one positive key and K negative keys for query q. This allows the model to learn a smaller distance for views from the same image and a larger distance between different images. IMPORTANTLY -  backpropagation is performed through the encoder network as can be seen in the first illustration but not through the momentum encoder The momentum encoder network is updated by a momentum-based moving average of the query encoder. This update is performed for every training iteration.",https://towardsdatascience.com/paper-explained-momentum-contrast-for-unsupervised-visual-representation-learning-ff2b0e08acfb,
Improved Baselines with Momentum Contrastive Learning,arXiv,https://arxiv.org/pdf/2003.04297.pdf,https://github.com/facebookresearch/moco,3/23/20,"In the MoCo paper, the unsupervised learning process is framed as a dictionary look-up: Each view or image is assigned a key, just like in a dictionary. This key is generated by encoding each image using a convolutional neural network, with the output being a vector representation of the image. Now, if a query is presented to this dictionary in the form of another image, this query image is also encoded into a vector representation and will belong to one of the keys in the dictionary, the one with the lowest distance."," In MoCo v2, this layer has been replaced by an MLP projection head, leading to better classification performance similar to Sim CLR. Another significant improvement has presented with the introduction of stronger data augmentations, also similar to those in SimCLR.",https://towardsdatascience.com/paper-explained-momentum-contrast-for-unsupervised-visual-representation-learning-ff2b0e08acfb,
An Empirical Study of Training Self-Supervised Vision Transformers,arXiv,https://arxiv.org/pdf/2104.02057.pdf,https://github.com/facebookresearch/moco-v3,8/16/21,"This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: selfsupervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other selfsupervised frameworks, with ablations in various aspects.",MocoV3 adapts the model to a different training process - vision transformers,,
sim CLR,arXiv,https://arxiv.org/pdf/2002.05709.pdf,,7/1/20,,"Purpose of a non-linear projection head: https://www.youtube.com/watch?v=tnktBNn7ygQ&t=3s - TLDR it makes the classification accuracy better because the MLP as opposed to a linear representation is more invariant to data augmentation, because it is a richer representation of the features of the image, this is what they call an ""expressive set of invariant representations mapped to z (z is the projection)"". This makes the calculated loss and the subsequent updating of the weights more fine-tuned in comparison to calculating the loss from the linear representation. | Example using t-sne to cluster representations https://medium.com/analytics-vidhya/understanding-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-d544a9003f3c","https://medium.com/@nainaakash012/simclr-contrastive-learning-of-visual-representations-52ecf1ac11fa#:~:text=Projection%20Head%3A%20A%20small%20neural,used%20in%20this%20projection%20head.",https://medium.com/syncedreview/pushing-the-limits-of-self-supervised-resnets-deepminds-relicv2-beats-strong-supervised-baselines-eeaa3709caa3
SwAV,arXiv,https://arxiv.org/abs/2006.09882,,1/8/21,,,,
Exploring Simple Siamese Representation Learning,arXiv,https://arxiv.org/pdf/2011.10566.pdf,SimSiam: Exploring Simple Siamese Representation Learning,11/20/20,"In a nutshell, this method can be thought of as “BYOL without the momentum encoder”. Unlike BYOL but like SimCLR and SwAV, our method directly shares the weights between the two branches, so it can also be thought of as “SimCLR without negative pairs”, and “SwAV without online clustering”.",,,
Revisiting Self-Supervised Visual Representation Learning,arXiv,https://arxiv.org/pdf/1901.09005.pdf,,1/25/19,"""Unsupervised visual representation learning remains
a largely unsolved problem in computer vision research.
Among a big body of recently proposed approaches for unsupervised learning of visual representations, a class of
self-supervised techniques achieves superior performance
on many challenging benchmarks. A large number of the
pretext tasks for self-supervised learning have been studied, but other important aspects, such as the choice of convolutional neural networks (CNN), has not received equal
attention. Therefore, we revisit numerous previously proposed self-supervised models, conduct a thorough large
scale study and, as a result, uncover multiple crucial insights. We challenge a number of common practices in selfsupervised visual representation learning and observe that
standard recipes for CNN design do not always translate
to self-supervised representation learning. As part of our
study, we drastically boost the performance of previously
proposed techniques and outperform previously published
state-of-the-art results by a large margin.""","Basically this paper explains why these papers typically use a final, linear evaluation (what they call ""downstream accuracy"" is actually computed from a logistic regression trained on the representations, which are generated by the self-supervised model.) They say that ""using a linear mdoel for evaluating the quality of a representation task is linearly separable in representation space. Tis is not necessarily prerequisite for useful representation. Further, using a more powerful model in the evaluation procedure might make the architecture choice for a self supervised taks less important. Hence, we conspider an alternative evaluation scenario where we use a multilayer perceptron for solving the evalutation task. The MLP provides only a marginal improvement over the linear evaluation and the relative performance of various settings is mostly un-changed. Thus, we conclude that the linear model is adequate for evaluation purposes."" Keep it simple stupid is the TLDR here.",,
Self-supervised learning and computer vision - Jeremy Howard,fast.ai,fast.ai/2020/01/13/self_supervised,https://keremturgutlu.github.io/self_supervised/,1/13/20,"What do you do if there are no pre-trained models in your domain? For instance, there are very few pre-trained models in the field of medical imaging. One interesting recent paper, Transfusion: Understanding Transfer Learning for Medical Imaging has looked at this question and identified that using even a few early layers from a pretrained ImageNet model can improve both the speed of training, and final accuracy, of medical imaging models. Therefore, you should use a general-purpose pre-trained model, even if it is not in the domain of the problem that you’re working in. However, as this paper notes, the amount of improvement from an ImageNet pretrained model when applied to medical imaging is not that great. We would like something which works better but doesn’t will need a huge amount of data. The secret is “self-supervised learning”. This is where we train a model using labels that are naturally part of the input data, rather than requiring separate external labels. In self-supervised learning the task that we use for pretraining is known as the “pretext task”. The tasks that we then use for fine tuning are known as the “downstream tasks”. Even although self-supervised learning is nearly universally used in natural language processing nowadays, it is used much less in computer vision models than we might expect, given how well it works. Perhaps this is because ImageNet pretraining has been so widely successful, so folks in communities such as medical imaging may be less familiar with the need for self-supervised learning. In the rest of this post I will endeavor to provide a brief introduction to the use of self-supervised learning in computer vision, in the hope that this might help more people take advantage of this very useful technique.",https://amarsaini.github.io/Epoching-Blog/jupyter/2020/03/23/Self-Supervision-with-FastAI.html,,