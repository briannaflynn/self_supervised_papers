TITLE,JOURNAL,LINK,GITHUB,DATE,SUMMARY,NOTES,PAPER_EXPLAINED,
Generalized radiograph representation learning via cross-supervision between images and free-text radiology reports,Nature Machine Intelligence,https://www.nature.com/articles/s42256-021-00425-9,,1/20/22,"""Pre-training lays the foundation for recent successes in radiograph analysis supported by deep learning. It learns transferable image representations by conducting large-scale fully- or self-supervised learning on a source domain; however, supervised pre-training requires a complex and labour-intensive two-stage human-assisted annotation process, whereas self-supervised learning cannot compete with the supervised paradigm. To tackle these issues, we propose a cross-supervised methodology called reviewing free-text reports for supervision (REFERS), which acquires free supervision signals from the original radiology reports accompanying the radiographs. The proposed approach employs a vision transformer and is designed to learn joint representations from multiple views within every patient study. REFERS outperforms its transfer learning and self-supervised learning counterparts on four well-known X-ray datasets under extremely limited supervision. Moreover, REFERS even surpasses methods based on a source domain of radiographs with human-assisted structured labels; it therefore has the potential to replace canonical pre-training methodologies.""","Uses a vision transformer - ""cross supervision"" approach means acquiring labels for supervised training from the radiology reports themselves, what they call ""free-text""",,
Pushing the limits of self-supervised ResNets: Can we outperform supervised learning without labels on ImageNet?,arXiv,https://arxiv.org/pdf/2201.05119.pdf,,1/13/22,,,,
GestaltMatcher facilitates rare disease matching using facial phenotype descriptors,Nature Genetics,https://www.nature.com/articles/s41588-021-01010-x,,2/10/22,,,,
Bootstrap your own latent: A new approach to self-supervised Learning,arXiv,https://arxiv.org/abs/2006.07733,,9/10/20,,,,
Momentum Contrast for Unsupervised Visual Representation Learning,arXIv,https://arxiv.org/pdf/1911.05722.pdf,https://github.com/facebookresearch/moco,3/23/20,"""Pre-training lays the foundation for recent successes in radiograph analysis supported by deep learning. It learns transferable image representations by conducting large-scale fully- or self-supervised learning on a source domain; however, supervised pre","ReLIC architecture incorporates features of this method - ""The keys are encoded by a slowly progressing
encoder, driven by a momentum update with the query encoder."" Since the goal of the model is learn to differentiate between a large number of different images, this query image encoding is not only compared to one mini-batch of encoded key images, but to multiple of them. To achieve that, MoCo forms a queue of mini-batches that are encoded by the momentum encoder network. | The loss resembles a softmax-based classfier loss that aims to classify dictionary query as key of query. The sum is computed over one positive key and K negative keys for query q. This allows the model to learn a smaller distance for views from the same image and a larger distance between different images. IMPORTANTLY -  backpropagation is performed through the encoder network as can be seen in the first illustration but not through the momentum encoder The momentum encoder network is updated by a momentum-based moving average of the query encoder. This update is performed for every training iteration.",https://towardsdatascience.com/paper-explained-momentum-contrast-for-unsupervised-visual-representation-learning-ff2b0e08acfb,
Improved Baselines with Momentum Contrastive Learning,arXiv,https://arxiv.org/pdf/2003.04297.pdf,https://github.com/facebookresearch/moco,3/23/20,"In the MoCo paper, the unsupervised learning process is framed as a dictionary look-up: Each view or image is assigned a key, just like in a dictionary. This key is generated by encoding each image using a convolutional neural network, with the output being a vector representation of the image. Now, if a query is presented to this dictionary in the form of another image, this query image is also encoded into a vector representation and will belong to one of the keys in the dictionary, the one with the lowest distance."," In MoCo v2, this layer has been replaced by an MLP projection head, leading to better classification performance similar to Sim CLR. Another significant improvement has presented with the introduction of stronger data augmentations, also similar to those in SimCLR.",https://towardsdatascience.com/paper-explained-momentum-contrast-for-unsupervised-visual-representation-learning-ff2b0e08acfb,
An Empirical Study of Training Self-Supervised Vision Transformers,arXiv,https://arxiv.org/pdf/2104.02057.pdf,https://github.com/facebookresearch/moco-v3,8/16/21,"This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: selfsupervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other selfsupervised frameworks, with ablations in various aspects.",MocoV3 adapts the model to a different training process - vision transformers,,
sim CLR,arXiv,https://arxiv.org/pdf/2002.05709.pdf,,7/1/20,,"Purpose of a non-linear projection head: https://www.youtube.com/watch?v=tnktBNn7ygQ&t=3s - TLDR it makes the classification accuracy better because the MLP as opposed to a linear representation is more invariant to data augmentation, because it is a richer representation of the features of the image, this is what they call an ""expressive set of invariant representations mapped to z (z is the projection)"". This makes the calculated loss and the subsequent updating of the weights more fine-tuned in comparison to calculating the loss from the linear representation. | Example using t-sne to cluster representations https://medium.com/analytics-vidhya/understanding-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-d544a9003f3c","https://medium.com/@nainaakash012/simclr-contrastive-learning-of-visual-representations-52ecf1ac11fa#:~:text=Projection%20Head%3A%20A%20small%20neural,used%20in%20this%20projection%20head.",https://medium.com/syncedreview/pushing-the-limits-of-self-supervised-resnets-deepminds-relicv2-beats-strong-supervised-baselines-eeaa3709caa3
SwAV,arXiv,https://arxiv.org/abs/2006.09882,,1/8/21,,,,
Exploring Simple Siamese Representation Learning,arXiv,https://arxiv.org/pdf/2011.10566.pdf,SimSiam: Exploring Simple Siamese Representation Learning,11/20/20,"In a nutshell, this method can be thought of as “BYOL without the momentum encoder”. Unlike BYOL but like SimCLR and SwAV, our method directly shares the weights between the two branches, so it can also be thought of as “SimCLR without negative pairs”, and “SwAV without online clustering”.",,,
Revisiting Self-Supervised Visual Representation Learning,arXiv,https://arxiv.org/pdf/1901.09005.pdf,,1/25/19,"""Unsupervised visual representation learning remains
a largely unsolved problem in computer vision research.
Among a big body of recently proposed approaches for unsupervised learning of visual representations, a class of
self-supervised techniques achieves superior performance
on many challenging benchmarks. A large number of the
pretext tasks for self-supervised learning have been studied, but other important aspects, such as the choice of convolutional neural networks (CNN), has not received equal
attention. Therefore, we revisit numerous previously proposed self-supervised models, conduct a thorough large
scale study and, as a result, uncover multiple crucial insights. We challenge a number of common practices in selfsupervised visual representation learning and observe that
standard recipes for CNN design do not always translate
to self-supervised representation learning. As part of our
study, we drastically boost the performance of previously
proposed techniques and outperform previously published
state-of-the-art results by a large margin.""","Basically this paper explains why these papers typically use a final, linear evaluation (what they call ""downstream accuracy"" is actually computed from a logistic regression trained on the representations, which are generated by the self-supervised model.) They say that ""using a linear mdoel for evaluating the quality of a representation task is linearly separable in representation space. Tis is not necessarily prerequisite for useful representation. Further, using a more powerful model in the evaluation procedure might make the architecture choice for a self supervised taks less important. Hence, we conspider an alternative evaluation scenario where we use a multilayer perceptron for solving the evalutation task. The MLP provides only a marginal improvement over the linear evaluation and the relative performance of various settings is mostly un-changed. Thus, we conclude that the linear model is adequate for evaluation purposes."" Keep it simple stupid is the TLDR here.",,
Self-supervised learning and computer vision - Jeremy Howard,fast.ai,fast.ai/2020/01/13/self_supervised,https://keremturgutlu.github.io/self_supervised/,1/13/20,"What do you do if there are no pre-trained models in your domain? For instance, there are very few pre-trained models in the field of medical imaging. One interesting recent paper, Transfusion: Understanding Transfer Learning for Medical Imaging has looked at this question and identified that using even a few early layers from a pretrained ImageNet model can improve both the speed of training, and final accuracy, of medical imaging models. Therefore, you should use a general-purpose pre-trained model, even if it is not in the domain of the problem that you’re working in. However, as this paper notes, the amount of improvement from an ImageNet pretrained model when applied to medical imaging is not that great. We would like something which works better but doesn’t will need a huge amount of data. The secret is “self-supervised learning”. This is where we train a model using labels that are naturally part of the input data, rather than requiring separate external labels. In self-supervised learning the task that we use for pretraining is known as the “pretext task”. The tasks that we then use for fine tuning are known as the “downstream tasks”. Even although self-supervised learning is nearly universally used in natural language processing nowadays, it is used much less in computer vision models than we might expect, given how well it works. Perhaps this is because ImageNet pretraining has been so widely successful, so folks in communities such as medical imaging may be less familiar with the need for self-supervised learning. In the rest of this post I will endeavor to provide a brief introduction to the use of self-supervised learning in computer vision, in the hope that this might help more people take advantage of this very useful technique.",https://amarsaini.github.io/Epoching-Blog/jupyter/2020/03/23/Self-Supervision-with-FastAI.html,,
,,,,,,,,