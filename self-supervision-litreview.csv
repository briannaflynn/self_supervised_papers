TITLE,JOURNAL,LINK,GITHUB,DATE,SUMMARY,NOTES,PAPER_EXPLAINED
Generalized radiograph representation learning via cross-supervision between images and free-text radiology reports,Nature Machine Intelligence,https://www.nature.com/articles/s42256-021-00425-9,,1/20/22,"""Pre-training lays the foundation for recent successes in radiograph analysis supported by deep learning. It learns transferable image representations by conducting large-scale fully- or self-supervised learning on a source domain; however, supervised pre-training requires a complex and labour-intensive two-stage human-assisted annotation process, whereas self-supervised learning cannot compete with the supervised paradigm. To tackle these issues, we propose a cross-supervised methodology called reviewing free-text reports for supervision (REFERS), which acquires free supervision signals from the original radiology reports accompanying the radiographs. The proposed approach employs a vision transformer and is designed to learn joint representations from multiple views within every patient study. REFERS outperforms its transfer learning and self-supervised learning counterparts on four well-known X-ray datasets under extremely limited supervision. Moreover, REFERS even surpasses methods based on a source domain of radiographs with human-assisted structured labels; it therefore has the potential to replace canonical pre-training methodologies.""","Uses a vision transformer - ""cross supervision"" approach means acquiring labels for supervised training from the radiology reports themselves, what they call ""free-text""",
Pushing the limits of self-supervised ResNets: Can we outperform supervised learning without labels on ImageNet?,arXiv,https://arxiv.org/pdf/2201.05119.pdf,,1/13/22,,,
GestaltMatcher facilitates rare disease matching using facial phenotype descriptors,Nature Genetics,https://www.nature.com/articles/s41588-021-01010-x,,2/10/22,,,
Bootstrap your own latent: A new approach to self-supervised Learning,arXiv,https://arxiv.org/abs/2006.07733,,9/10/20,,,
Momentum Contrast for Unsupervised Visual Representation Learning,arXIv,https://arxiv.org/pdf/1911.05722.pdf,https://github.com/facebookresearch/moco,3/23/20,"""Pre-training lays the foundation for recent successes in radiograph analysis supported by deep learning. It learns transferable image representations by conducting large-scale fully- or self-supervised learning on a source domain; however, supervised pre","ReLIC architecture incorporates features of this method - ""The keys are encoded by a slowly progressing
encoder, driven by a momentum update with the query encoder."" Since the goal of the model is learn to differentiate between a large number of different images, this query image encoding is not only compared to one mini-batch of encoded key images, but to multiple of them. To achieve that, MoCo forms a queue of mini-batches that are encoded by the momentum encoder network. | The loss resembles a softmax-based classfier loss that aims to classify dictionary query as key of query. The sum is computed over one positive key and K negative keys for query q. This allows the model to learn a smaller distance for views from the same image and a larger distance between different images. IMPORTANTLY -  backpropagation is performed through the encoder network as can be seen in the first illustration but not through the momentum encoder The momentum encoder network is updated by a momentum-based moving average of the query encoder. This update is performed for every training iteration.",https://towardsdatascience.com/paper-explained-momentum-contrast-for-unsupervised-visual-representation-learning-ff2b0e08acfb
Improved Baselines with Momentum Contrastive Learning,arXiv,https://arxiv.org/pdf/2003.04297.pdf,https://github.com/facebookresearch/moco,3/23/20,"In the MoCo paper, the unsupervised learning process is framed as a dictionary look-up: Each view or image is assigned a key, just like in a dictionary. This key is generated by encoding each image using a convolutional neural network, with the output being a vector representation of the image. Now, if a query is presented to this dictionary in the form of another image, this query image is also encoded into a vector representation and will belong to one of the keys in the dictionary, the one with the lowest distance."," In MoCo v2, this layer has been replaced by an MLP projection head, leading to better classification performance similar to Sim CLR. Another significant improvement has presented with the introduction of stronger data augmentations, also similar to those in SimCLR.",https://towardsdatascience.com/paper-explained-momentum-contrast-for-unsupervised-visual-representation-learning-ff2b0e08acfb
An Empirical Study of Training Self-Supervised Vision Transformers,arXiv,https://arxiv.org/pdf/2104.02057.pdf,https://github.com/facebookresearch/moco-v3,8/16/21,"This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: selfsupervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other selfsupervised frameworks, with ablations in various aspects.",MocoV3 adapts the model to a different training process - vision transformers,